# Optimizing ARIA’s Context and Memory for GPT-OSS 20B

## Overview of ARIA Agent Architecture

ARIA – Space Rider Mission Control Agent is an offline, memory-augmented reasoning system designed to assist with parafoil landings. It uses an open-source 20B parameter model (GPT-OSS-20B) running locally (on Groq hardware or CPU/GPU) to analyze telemetry and suggest landing actions. The architecture emphasizes retrieval-augmented context and a human-in-the-loop for safety. ARIA demonstrates how a small local model can retrieve procedures and past lessons, reason over live data, and propose safe, auditable next actions with risk assessments. Crucially, the agent learns across missions by storing episodic logs and distilling them into semantic “lessons,” enabling improved performance on subsequent runs without any fine-tuning. This approach aligns with modern context engineering principles ￼, where we systematically optimize the information given to the LLM (via retrieval, processing, and memory management) to maximize its performance. ARIA’s fully offline and local operation is a key feature (addressing the “Best Local Agent” goal), ensuring it can function during communication gaps and be deployed on secure infrastructure for space missions.

## Execution Sequence of the ARIA Agent

To understand how ARIA works, let’s map out the sequence of execution steps and highlight how the memory fabric is involved at each stage:
1.	Scenario Initialization: A mission scenario (e.g. a specific landing with certain wind conditions) is started via the Playback Service. The system resets or initializes the memory stores for a new episode (clearing any transient data from previous runs while retaining long-term knowledge). Any relevant mission documents (flight manuals, procedures) are pre-ingested into the local database if not already done. Environment settings (e.g. using Groq accelerated inference or fallback to local CPU) are loaded via settings.py.
2.	Real-Time Telemetry & Event Detection: The Playback Service streams simulated telemetry data from a CSV at real-time speed (e.g. 20 Hz). This includes altitude, velocity, wind, etc. A separate Events module monitors this stream and triggers 1 Hz “reasoning ticks.” On each tick (once per second of simulation), it may also detect anomalies or phase changes (e.g. entering final approach, excessive crosswind, sensor dropout). These events are logged to the episodic memory (with timestamps) and also forwarded to the planning module as part of the context. For example, if a sharp crosswind is detected, an event like “Anomaly: high crosswind at 300m altitude” is recorded, which will cue the agent to address it in the plan.
3.	Decision Cycle (1 Hz Planning Loop): Every tick, the ARIA Planner (services/planner.py) is invoked. This is the core loop where the agent decides on the next action. The planner gathers the latest world state: current telemetry, any active anomalies or status (like “crosswind caution”), and the recent history of actions. These are important for situational awareness in the prompt. Each tick essentially represents a multi-turn interaction in which the agent has memory of previous decisions and events, allowing it to produce a coherent next step that accounts for what has already happened.
4.	Context Assembly for the Planner: Before calling the GPT-OSS model, ARIA must construct an input prompt that includes all relevant context. This is handled by the Memory Fabric components (aria/memory/retriever.py and composer.py):
* The episodic memory (short-term store) is queried for the most recent events and decisions (for example, the last few seconds of telemetry or last N logged events). This ensures continuity - the model knows what state it’s coming from. Rather than raw high-frequency data, this is a distilled summary (e.g. “Altitude dropped from 500m to 400m in last 5s, wind steady from west at 5 m/s”). Recent critical events (like “lost GPS signal 10s ago”) are included verbatim.
* The documentation memory is queried (using full-text search or embeddings) for any procedures or reference info relevant to the current phase of flight. For instance, if the agent is in the flaring phase, the retriever might pull a snippet from the Space Rider manual about proper flare technique or acceptable wind limits for landing. This is classic Retrieval-Augmented Generation (RAG) – the agent grounds its decisions in domain knowledge.
* The semantic memory (lessons) is also queried via a similarity search. ARIA will look for prior missions or training examples that resemble the current situation. For example, if a previous mission handled a high crosswind landing, a distilled lesson from that run (e.g. “Lesson from Mission 5: In strong crosswinds, apply a flare 10m earlier than normal to compensate for drift”) might be retrieved. This gives the model heuristics learned from experience.
* All these pieces – recent episodic context, relevant doc snippets, and similar past lessons – are combined by the composer into the prompt. They are typically inserted into the prompt after a system message or as part of a structured template. ARIA’s prompts.py defines a system prompt with role instructions (e.g. to output a JSON plan with certain fields) and possibly a few-shot example of a decision-making turn (so the model knows the format). Then the retrieved context is appended. The result is a compact, information-rich prompt that frames the model’s task for this tick. Notably, ARIA’s design ensures this prompt stays within the token limit of GPT-OSS-20B by selecting only the most relevant data and summarizing where needed – this token budget management is handled by a governor component. If the context is too large, the governor may drop the least relevant pieces or truncate/summarize older parts to fit the model’s context window.
5.	Model Plan Generation (via GPT-OSS-20B): With the prompt ready, the planner calls the GPT-OSS model through an OpenAI-compatible interface (aria/agent.py). This could be a local inference call (to a Groq chip or a CPU/GPU model server) but is abstracted so that the rest of the system doesn’t depend on the hardware specifics. GPT-OSS-20B, given the carefully engineered prompt, will generate a proposed Action Plan. We instruct the model to output this plan in a structured format (likely JSON ) containing fields like: the decision/action to take next (e.g. “Begin flare maneuver”), a brief reasoning or justification (which makes the decision auditable), an assessment of risks (what could go wrong) and confidence level in the success of this action. By asking the model to think through risk and confidence, we encourage it to consider edge cases, which is important for safety. The model’s output might look like:
```json
{
  "action": "Initiate flare",
  "reasoning": "Altitude is 50m and within optimal flare window. Winds are high but steady; flaring now prevents hard landing.",
  "risk": "Moderate – if flared too early, could lose lift; but delay risks ground impact due to wind drift.",
  "confidence": 0.85
}
```
This structured output ensures the next components can easily parse and use the information. Under the hood, GPT-OSS-20B is leveraging all the context we provided – including any retrieved lesson (like flaring 10m early in strong wind) – to inform this plan. This is how in-context learning manifests: the model adapts its output based on examples and data in the prompt, effectively learning from them on the fly . It’s worth noting that research has shown LLMs can implicitly incorporate new information from prompts as if updating their parameters, improving performance without any gradient-based fine-tuning. ARIA capitalizes on this by injecting lessons and reference data each time.

6.	Safety Gate and Output Validation: Once the model provides a plan, ARIA’s Safety Gate (services/safety_gate.py) intervenes to double-check it before presenting to the human. This module contains rule-based checks (“red lines”) for flight safety parameters – for example, it knows the maximum bank angle allowed, the safe range for crosswind during landing, the proper altitude window for flaring, and so on. It evaluates the proposed action against these constraints. If the model’s suggestion violates any safety limit, the Safety Gate can flag it or adjust the plan. It may also adjust the confidence metric in the plan via a confidence fusion logic: for instance, if the model was confident but suggested something outside a known safe envelope, the system might downgrade the confidence or mark the plan as high-risk. Conversely, if the model’s plan passes all safety checks and matches known best practices, the system can be more confident. This step is critical because LLMs, especially smaller ones, can occasionally hallucinate or err – having a domain-specific guardrail ensures we don’t blindly follow the AI. The output of this stage is a vetted plan, e.g. the same JSON but now with a revised risk level or an added note if something was borderline. (If a plan were completely unsafe, the system could even override it or ask the model for an alternative, but typically we expect the few-shot examples and retrieved procedures to guide the model away from grossly unsafe actions in the first place.)
7.	Human Review and Execution: The vetted plan is then sent to the Human-AI UI frontend (e.g. the PlanPanel.tsx component). The UI displays the suggested action and its rationale, along with the risk and confidence assessments in a clear format (a “plan card”). At this point, the human operator (the flight controller) has three choices: Approve, Modify, or Reject the plan.
* If Approved, the suggested action is carried out (in the simulation, this might just mean logging that the action was taken – since the scenario is pre-recorded, the “execution” might not alter telemetry, but in a real system this would send a command to the vehicle). The timeline (via Timeline.tsx) is updated with this decision event (e.g. “✅ Initiated flare at 45m (approved by human)”). Metrics like touchdown hardness, drift, etc., will later reflect the outcome of this decision.
* If Modified, the human can tweak the plan – perhaps changing a parameter (e.g. “start flare at 40m instead of 50m”) or adding a note. This modified plan is then executed. The modification could be fed back into the episodic memory as well, so the agent knows the human’s input (and potentially learns from the human’s correction). The timeline notes that a modification was made.
* If Rejected, the human essentially tells the agent “no, we won’t do that.” In this case, either the human will take manual control or ask the agent for a different suggestion. ARIA can log the rejection event and possibly query the model again (with that feedback) for an alternative plan. The UI might allow the user to request a re-plan or simply proceed without AI assistance for that tick. Rejections are valuable data - they indicate the model’s suggestion was suboptimal. ARIA could use that information in hindsight to avoid similar suggestions (this could be part of the learning process, e.g. storing that the suggestion was rejected and why).
8.	Memory Logging of Outcome: After the human decision, the cycle continues. If an action was approved/executed, the next few seconds of telemetry will show the outcome of that action (e.g. the flare was done – did the descent rate reduce? Did the craft stabilize?). The Metrics module (services/metrics.py) calculates outcome measures continuously (like peak forces, distance from target, etc.). Significant outcomes (like “Touchdown velocity = X m/s, within safe range”) or any deviations are logged to episodic memory. This provides a record that “we did action A at time T, and outcome was B.” If the scenario is still ongoing (not landed yet), the next tick will include these recent outcomes in the context, closing the feedback loop for the model (the model will see “last action had such effect” and can adjust next recommendations accordingly). This logging continues until the mission ends (e.g. touchdown). The episodic memory by the end contains a chronological transcript of the scenario: all telemetry events of note, all model suggestions, and all human decisions.
9.	Episodic-to-Semantic Distillation: Once a mission scenario concludes, ARIA performs a post-mission analysis. This is handled by the Distiller (aria/memory/distill.py). The idea is to compress the rich episodic log into a semantic memory entry – a “lesson learned” from this run. ARIA can either use a simple heuristic or invoke the GPT-OSS model itself to help summarize. For example, the distiller might prompt the model with the entire episodic log and ask: “Summarize the key decisions and outcomes of this mission, and note any important lessons for future similar missions.” The output could be a paragraph like: “Mission X: Faced high crosswinds. The AI recommended early flare which led to a gentle touchdown (peak force below threshold). Lesson: In strong crosswinds, flaring ~5–10m early is effective to reduce landing impact. ￼”. This summary is then stored in the semantic memory database, indexed by scenario characteristics (tags like “crosswind”, “night landing”, “comms blackout” etc.). We do this because it’s not feasible to carry over the entire episodic log into future missions (too much detail), but a concise lesson or two is very valuable. Over time, as more scenarios run, the semantic memory grows into a knowledge base of best practices and pitfalls. Notably, all of this learning is happening without retraining the model – it’s purely via accumulating context that we feed back in later. This approach leverages the model’s capacity for long-term in-context learning, effectively giving it new “training data” in the form of these stored experiences that it will see in future prompts.
10.	Continuous Improvement Across Missions: When a new mission scenario begins, ARIA’s retriever will pull not only relevant docs but also any past lessons from similar missions to include in the prompt (as described in step 4). This means on mission #2 (even the very next run), the agent is already better informed. For example, if the first run taught a lesson about crosswinds, the second run’s prompt will likely contain that lesson if similar conditions are detected, leading the model to adjust its plan accordingly. The expectation (and a key demo goal) is that the second run is measurably safer or more optimal than the first, thanks to this memory augmentation. We essentially achieve a form of retrospective improvement: the agent reflects on outcomes and adapts its strategy next time. In practice, this could reduce landing impact or avoid a previously encountered issue (like a late flare) on subsequent trials. Over many runs, ARIA could accumulate a robust set of strategies for a variety of conditions, all stored in its memory fabric.

Throughout this loop, ARIA supports multi-turn interaction naturally. The “turns” here are the 1 Hz planning cycles (and any human queries or modifications). Because the agent always considers the recent interaction history (episodic memory), it remembers previous human inputs and its own prior reasoning. For instance, if a human asked a question two minutes ago (“Why can’t we delay the flare further?”) and the agent answered it, that Q&A would be in the log and thus the agent wouldn’t repeat the explanation unless needed, and it would incorporate that feedback (maybe the human was concerned about something, so the agent factors that into future plans). This continuity is vital for trust and usability in human-AI collaboration.

## Memory Fabric and Context Management

Illustration of a MemGPT-inspired memory hierarchy: an LLM with a fixed context window manages a working context (short-term memory) in the prompt, while reading/writing additional information to external storage via special tools or calls. “Archival storage” holds long-term knowledge, and a queue mechanism controls what enters or leaves the prompt context. ARIA’s memory fabric implements a similar multi-tier approach, dividing information between in-prompt context and a persistent SQLite store.

ARIA’s Memory Fabric is the backbone that enables the agent to provide extended and relevant context to GPT-OSS-20B beyond what the raw context window would normally allow. It consists of multiple components, each handling a different “tier” of memory, together with logic to manage what information is in the model’s immediate context vs. what is stored externally (and how to retrieve it on demand). Key parts of this memory system include:
* Episodic Memory (Short-Term Log): This is essentially a high-fidelity transcript of the current mission. Implemented as a SQLite database (with FTS5 full-text search), it records timestamped events: state summaries, anomalies detected, decisions made, and their outcomes. Think of it as the agent’s running memory of this episode only. It is read-write and grows as the mission progresses. The purpose of episodic memory is to allow the agent to recall what just happened a few seconds or minutes ago (within this run) even if that detail can’t all fit in the prompt at once. The retriever can query this log for recent relevant entries (e.g. the last N events, or events matching certain keywords like “flared” or “error”). By using FTS search, the agent can also retrieve specific details on the fly (like “when was the last wind reading above 10 m/s?”) if needed. This design is analogous to MemGPT’s working context or a “recall buffer” that holds recent interactions ￼, except in our case the recall is mediated by explicit queries rather than the model’s own tool-use (for now).
* Semantic Memory (Long-Term Lessons): This is a growing knowledge base of distilled knowledge. Each entry is a compact representation of a past episode’s key takeaway, stored with metadata and possibly an embedding for similarity search. The semantic memory is read-only during prompt assembly (only written to by the distillation process after each mission). It serves as the agent’s long-term memory – facts or heuristics learned over time. Because it’s indexed semantically, when a new situation arises, we can do a nearest-neighbor search (via embeddings or even via FTS keywords) to find if any past lesson “sounds like” it might apply. For example, if today’s landing has strong crosswinds and we have two past lessons about crosswinds, those lessons will likely surface. Only the most relevant one or two are inserted into the prompt to avoid overloading. This mechanism gives ARIA an ability to generalize from past specific experiences, which is crucial for adapting across missions. In MemGPT/Letta terms, this semantic store is akin to an archival memory – kept outside the immediate context, but fetched when needed ￼ ￼. The use of a vector database or embedding index for this purpose is suggested by MemGPT’s design (they used a vector DB for archival memory) ￼, and we can implement similarly (e.g. store embeddings of each lesson for quick cosine similarity search).
* Documentation / Knowledge Base: ARIA also has a memory for static domain knowledge – the Space Rider manuals, checklists, and any reference documents that have been ingested (via tools/docs_ingest.py). This can be considered another form of long-term memory, but one that doesn’t evolve (read-only data). It’s stored in SQLite with FTS5 (and possibly with embeddings for semantic search as well, if we processed the docs). When the agent needs factual information or standard procedures, it comes from here. In context-engineering terms, this is part of the retrieval-augmented generation (RAG) system ￼. By querying the docs for relevant sections (using keywords like “flare altitude” or “crosswind limit”), ARIA ensures the model’s output is grounded in authoritative information instead of making something up. The retrieval is done automatically by the retriever component before invoking the model. Each retrieved snippet (maybe a few sentences each) is added to the prompt, often prefaced or formatted in a way that the model knows it’s reference material (some designs use a citation style or a delimiter to indicate “here’s an excerpt from manual”). This gives the model confidence to make recommendations aligned with official guidelines and also makes the output auditable (the human can see if the suggestion corresponds to a known procedure).
* 	Working Memory (Prompt Context Buffer): While episodic/semantic/doc memory are about storage and retrieval, the working memory is essentially the subset of information that we actually place into the LLM’s prompt on a given call. You can think of it as a “scratch pad” or the in-context memory that the model can see at inference time ￼. ARIA’s composer builds this working context by selecting from the sources above. Typically, it will include: (a) a summary of the current state (from telemetry – either directly or via a short algorithmic summary), (b) the last few critical events or actions (from episodic memory), (c) the top relevant doc snippet (if any), and (d) the top relevant lesson from past runs. This working memory, combined with the fixed system prompt and instructions, forms the content of the finite context window that GPT-OSS-20B processes. Managing this working memory is crucial – it’s a limited resource (e.g. GPT-OSS-20B might have a context length of ~2048 tokens, as an assumption). That’s why we have the memory governor logic. If the agent is in a very complex scenario where a lot seems relevant, we can’t just dump everything into the prompt. The governor will prioritize: for instance, recent critical telemetry and any directly applicable lesson will have higher priority, whereas an older event or a tangential document excerpt might be dropped if we’re at risk of context overflow. We might also do some context compression: e.g. instead of listing every small event over the last 60 seconds, compress them into a higher-level summary (“Descent stable, small oscillations, no new anomalies in last 60s”). This approach aligns with general findings that optimizing the information payload is key to LLM performance ￼.
* 	Memory Management & Tiered Storage: The combination of the above elements allows ARIA to function like a basic implementation of the MemGPT/Letta “LLM OS” concept. We have effectively a two-tier memory: the fast tier is the prompt context itself (system prompt + working memory), and the slow tier is the external SQLite stores (episodic logs, semantic DB, docs) which can be arbitrarily large. ARIA moves data between these tiers by retrieving relevant pieces into the fast tier when needed, and by offloading information out of the prompt (either by not including it, or by summarizing it) when it’s not immediately needed. This mimics virtual context extension – giving the illusion of a larger context window than the model actually has ￼. As the MemGPT paper notes, an OS-like memory manager can enable an LLM to handle tasks far beyond its normal context limit by judiciously swapping information in and out ￼. In ARIA’s case, if we had a very long mission or very large documents, we could still manage by processing chunks at a time with the model, rather than trying to feed everything at once. The current design retrieves what’s relevant for the immediate decision, which is usually sufficient. If the agent ever needs something that wasn’t retrieved initially, we could either retrieve it on-the-fly (if the model asks or if a later tick finds it relevant) or design the model prompt to explicitly request more info if needed (more on this in ReAct approach below).
* 	Updating Memory: One aspect of memory management is not just reading from it but writing to it appropriately. ARIA writes to episodic memory in real-time (every event/decision). It writes to semantic memory only at the end of an episode (to add a lesson). There’s also the possibility of the model itself contributing to memory: e.g. the model’s reasoning or any self-reflection could be stored. In our current architecture, we largely store final decisions and outcomes, not the model’s entire chain-of-thought (to conserve space and because the chain-of-thought can be quite verbose). However, storing a distilled form of the reasoning (“why did we do what we did”) could be useful for later analysis. That is somewhat captured in the rationale that the model outputs for each plan. Indeed, the plan’s “reasoning” field is effectively the model’s summarized chain-of-thought for that decision. We do keep that as part of the episodic log. So in semantic memory, a lesson might include the rationale behind the action, which helps the model next time understand why that action was or wasn’t good. This contributes to more transparent long-term memory – a design goal highlighted by frameworks like Letta ￼ (which emphasize keeping memory that the model can understand later).

In summary, ARIA’s memory fabric ensures the agent always has the information it needs at hand, without being limited by the fixed context size of the model. It combines recent, situational data with stored knowledge and documentation to form a comprehensive picture each time the model is queried. By managing this pipeline of information through retrieval and summarization, ARIA avoids both the context overflow problem and the hallucination problem (since the model is less likely to make things up if we supply the real data). This memory-centric approach is at the heart of ARIA’s innovative design and is what enables a local model to perform at a high level on a complex, temporally extended task like a landing procedure.

## Advanced Context Engineering Techniques for ARIA

To push ARIA’s performance and efficiency further, we consider several advanced techniques from recent research in context management, reasoning, and optimization. Incorporating these can make GPT-OSS-20B’s usage more efficient and its decision-making more robust:
* ReAct (Reasoning + Acting) Approach: The ReAct framework proposes that an LLM should generate reasoning traces and task-specific actions interleaved ￼. In practice, this means instead of the model always producing a direct answer (plan) in one go, it would first output a thought process (chain-of-thought) and possibly perform intermediate steps (like tool use), then arrive at the final action recommendation. We have implicitly given the model some reasoning space by asking it to output a “reasoning” or rationale with each plan. However, we could formalize a ReAct style interaction as follows: the model’s output could be structured with a “Thought:” and “Action:” sequence. For example, it might say (not to the user, but as an internal trace) Thought: The wind is higher than last time; recall what happened previously. then Action: retrieve_lesson(“crosswind”). The system would execute that action (retrieve the lesson from memory and feed it back in), and then the model continues Thought: Got the lesson about early flare. Given that… then Action: plan = Initiate flare earlier…. This interleaving allows complex queries or decisions to be broken down. In current ARIA, because each cycle’s task is relatively narrow (make one decision for the next few seconds), we might not need a lengthy chain-of-thought. But for more complex queries (like if the user asks “Plan the entire mission profile” or “Explain how you would handle multiple failures at once”), a ReAct approach can improve coherence. Additionally, ReAct has been shown to reduce hallucinations by grounding reasoning with actions (like actual info retrieval) ￼. For ARIA, implementing ReAct would involve parsing the model’s output for special tokens or patterns indicating an action (like a pseudo-API call). The tools available could include memory queries, calculation utilities, or even asking for human input if stuck. Introducing ReAct would increase complexity but could make the agent more intelligent in how it uses its resources (it would no longer be limited to what the retriever pre-fetched – it could ask for something else if needed). It also aligns with making the system interpretable: the reasoning trace is visible, so humans can follow how the AI arrived at a suggestion, increasing trust ￼. Given ARIA’s focus on safety and human collaboration, having an explicit reasoning trace (which we partially have via the “reasoning” field) is beneficial. We just have to ensure any such trace doesn’t confuse the user – likely we keep detailed chain-of-thought hidden or in logs, and only show summarized reasoning to the user.
* Chain-of-Draft for Efficient Reasoning: Chain-of-Thought prompting often boosts accuracy but at the cost of verbosity (the model writing out long explanations). The Chain-of-Draft (CoD) idea offers a more efficient alternative: let the model generate minimalistic intermediate drafts of its reasoning ￼. By focusing only on essential information in these drafts, CoD achieves similar or better accuracy than verbose CoT while using a tiny fraction of the tokens (as low as 7.6% of tokens compared to CoT) ￼. We can leverage this concept in ARIA to optimize performance. For instance, instead of having the model produce a long-winded rationale every single tick (which could be slow and sometimes unnecessary), we can prompt it to keep its reasoning brief and to-the-point – essentially a “draft” of reasoning that is just sufficient to justify the action. We already nudge it to output concise JSON with short fields. We can refine the prompt instructions or few-shot examples to demonstrate very terse yet informative reasoning. If needed, we can also do a multi-step draft: the model first internally (or in a hidden part of the prompt) comes up with key points, then composes the final answer. However, since ARIA’s decisions are fairly short-form by nature, simply emphasizing brevity might suffice. The benefit of CoD would be reduced latency and token consumption ￼ – important for real-time operation at 1 Hz and for running on local hardware. By “thinking faster by writing less,” ARIA can make decisions quicker and leave more headroom in the context window for critical data rather than long self-talk. We should validate that the concise output still retains accuracy (according to Chain-of-Draft research, it should, if done right ￼). Practically, we might implement this by giving the model an upper bound on tokens for each field, or by using a style guideline like: “Provide reasoning in at most 2 sentences” in the prompt. The governor can also enforce truncation of overly verbose outputs if necessary.
* Adaptive Model/Tool Routing (Budget-Aware Reasoning): ARIA is built around a single 20B model, but we can draw inspiration from the idea of adaptive routing ￼ to make the system both more efficient and more resilient. In research, adaptive LLM routing means dynamically choosing which model or method to use for each query to balance cost and performance ￼. In our context, this could translate to: if a situation is straightforward and our rule-based Safety Gate or some simpler logic can handle it, we might not call the big LLM at all for that tick – saving time. For example, during a nominal descent with no anomalies, the next action might be obvious (e.g. “continue on current glide path”) which could be encoded as a simple rule. We could set a policy: only invoke GPT-OSS for non-trivial decisions or when an anomaly is present. Conversely, if something very critical or novel happens, we ensure the full model is engaged. Another angle is using a smaller model or distilled model for certain tasks. Perhaps we could have a faster lightweight model that’s 7B parameters to do quick routine checks or generate an initial draft plan, and only use the 20B for validating or refining it. This is akin to using a cascade of LLMs, where the smaller one handles easy queries and the larger one handles hard queries – a form of routing that could dramatically cut computation while retaining high performance ￼. In fact, a study showed that intelligent routing can achieve ~93% of a large model’s performance at only 1/4 of the cost by not always using the largest model ￼. For the hackathon demo, we likely stick to one model due to complexity, but we can design the architecture with this extensibility in mind. For instance, our planner.py could have a mode where it first runs a cheap check: “Is this situation within normal bounds? If yes, maybe just use a stored optimal action or a very small model; if not, call GPT-OSS.” The modular design (services and memory fabric decoupled from the model backend) would allow plugging in such logic later. Additionally, budget constraints might include response length – if running on CPU, perhaps we limit the model’s output tokens to ensure <1s response. The chain-of-draft approach already helps with that by limiting verbosity. We could also explore partial processing: e.g., compute embeddings of telemetry and let a linear model or simpler ML predict an action, using the LLM only to provide reasoning or to validate that action. These are speculative optimizations, but they illustrate that ARIA’s framework can accommodate sophisticated strategies to use resources efficiently while maintaining safety.
* Long-Term Autonomy and Self-Reflection: Beyond just learning lessons, we could give ARIA a more proactive self-improvement capability. One concept is Retrospective Reflection (sometimes called “Reflexion” in research) where after completing a task, the agent asks itself what could be improved next time. We partially do this by summarizing lessons, but we could formalize it by having the model itself generate a brief critique of its performance after each mission. For example, after a run, prompt GPT-OSS with a question: “Looking at the mission log, were there any suboptimal decisions by ARIA? What should ARIA do differently in the future?” This might produce insights like, “I notice ARIA suggested a flare a bit late; next time in high winds, flaring earlier would be better.” We can then store this insight as a special kind of memory (like a “self-reflection” note). This is similar to how humans debrief after an exercise to improve next time, and how some LLM agents in research have improved success rates by reflecting on failures and avoiding them later. Moreover, we can integrate such reflections into the semantic memory so that the agent’s future behavior directly benefits. The advantage of doing this with the LLM is that it can catch subtleties that a simple metric might not (maybe an action succeeded but was riskier than necessary, etc.). Essentially, ARIA can simulate “learning from critique” in addition to learning from outcome. This makes the agent adapt across missions not just in a reactive way, but in a proactive, cognitive way – it’s analyzing its own decision patterns for flaws and adjusting strategy.
* Multi-modal and Additional Tool Integration: Currently ARIA deals with primarily textual/numeric data (telemetry, text manuals). In future enhancements, we could integrate other data types – for instance, if there were images (like descent camera) or maps, those could be processed by specialized models and provided as textual descriptions to the LLM. The memory fabric could then also store those processed forms. We might also incorporate more tools: e.g., a physics simulation tool that the model could call to estimate outcome of an action, or a trajectory predictor. These would be akin to the “tools” in LLM frameworks. The architecture already separates an events and metrics service, which are domain-specific tools outside the LLM. We could allow the LLM to explicitly request a metric (like “Tool: simulate landing if flare now”) which the metrics.py could handle, returning a result for the model to consider. This crosses into the idea of the model as a high-level orchestrator, leveraging both memory and tool use to solve tasks – essentially building an LLM-centric agent that can perceive (via memory), think (via chain-of-thought), and act (via tool APIs). Many open-source agent frameworks (LangChain, etc.) facilitate this, and ARIA’s design is quite compatible given its modularity and the Letta/MemGPT-inspired structure.

Incorporating the above techniques would mark ARIA as an example of state-of-the-art context-aware AI agent design. It would showcase how a local model, empowered by intelligent memory and reasoning strategies, can achieve complex autonomous behavior that improves over time. We already have the foundation; these additions would refine and optimize the agent’s capabilities.

Integration Plan for Memory and Context Optimization

To implement the above ideas in the context of ARIA’s existing structure, we outline a concrete plan:
1.	Extend Memory Schema and Retrieval: We will verify that the SQLite schema (aria/memory/schema.sql) has tables for episodic data, lessons, and docs. If not, we add them. We’ll also include metadata columns (e.g. scenario type, tags like “crosswind”) for each lesson to facilitate targeted search. In retriever.py, we integrate an embedding-based search for both docs and lessons (using a lightweight embedding model) in addition to the FTS keyword search. For example, we can generate embeddings for each lesson summary and store them (perhaps using a library or a simple cosine similarity computation at query time). This hybrid retrieval (embedding + keyword) ensures high recall of relevant info. We also adjust retriever to return not just raw text, but maybe a structured snippet indicating source (so the prompt can say “According to Lesson #5: …” or “Manual says: …”, giving the model clarity on origin of info).
2.	Improve Context Composer Logic: In composer.py, we’ll implement more advanced context assembly rules. For instance, define a sliding window summary for telemetry: maintain a rolling summary state that updates every tick (so we don’t have to include a bunch of past readings, just one state object). We can also mark certain memory items as “critical” (must include) vs “nice-to-have”. E.g., an active emergency or a direct human instruction must always go into the prompt. We incorporate the token budget awareness: count tokens of candidate context items and drop lowest priority ones if we exceed the limit. We can also compress the reasoning from previous steps: maybe only the last one or two decisions’ rationales are included fully, while older ones are omitted or summarized (the model probably doesn’t need to see every rationale from the whole run each time). Essentially, we formalize a priority list: Current state > Active anomaly > Last decision outcome > Key lesson > Relevant doc > Older events…, trimming from the bottom as needed. This keeps context lean and focused.
3.	Prompt Engineering for JSON & Brevity: In prompts.py, we will refine the system and few-shot prompts. We ensure there is a mode for instructing GPT-OSS to output JSON only. Possibly, we use the OpenAI function calling format or just specify a rigid template. The few-shot example will demonstrate exactly how to fill the fields with concise text. To implement Chain-of-Draft style brevity, our examples and instructions will emphasize short reasoning. We might include a rule like: “Reasoning should be one or two sentences focusing only on key factors. Do not ramble.” The model, seeing such guidance, will likely comply (especially if we give an example of verbose vs concise and show the concise being the expected format). We’ll test a few outputs to ensure it’s still informative. For any user-facing natural language answers (e.g. if user asks a question in chat), we can have a separate prompt template or a flag indicating that. Our design could have the UI/front-end specify the request type (plan-generation vs chat), and the backend chooses the appropriate prompt. This way the agent can smoothly shift between structured output and freeform explanations as needed.
4.	Safety Gate Enhancements: While not directly memory, optimizing context also involves making sure the model doesn’t need to handle everything. The Safety Gate can be extended to handle more conditions so that the model’s context can sometimes exclude certain checks. For example, if the Safety Gate monitors a parameter (like descent rate) and will interject if it’s dangerous, the model might not need to devote tokens to reasoning about that (knowing that the safety net is there). We essentially offload some concern from the model’s context. We might also log Safety Gate interventions into memory (so the agent knows a suggestion was altered or flagged – that’s useful meta-information).
5.	Implement ReAct Lite: To dip our toes into ReAct without overhauling everything, we could implement a simple loop in planner.py: after getting the model’s output, check if it contains a special token or phrase that indicates a follow-up action rather than a final plan. For instance, we could decide that if the model outputs something like "action": "query_manual", "query": "some question" in the JSON (we’d have to allow it in the output schema), then the planner will fulfill that query (search the docs for “some question”), add the answer to context, and call the model again to get the real plan. This would allow one tool-use step. We can include a hidden example of this in few-shot (or documentation) so the model knows it can ask. If we don’t have time to implement parsing, an easier alternative: if the model’s reasoning says something like “I am not sure, I should check the manual,” we could catch that pattern. This is a bit brittle though. Ideally, tool use should be explicit. Given hackathon time, we might mention this as a design capability even if not fully implemented – the judges will see the architecture can support it. The Letta framework, if we chose to use it, might handle a lot of this out-of-the-box, but since we already have a custom memory system, integrating Letta directly might not be feasible last-minute. Instead, we borrow concepts from it (which we’re doing).
6.	Chain-of-Draft Prompting: We will test a two-pass approach for any heavy-duty queries (like “plan entire mission” requests). For example, when the user asks for a full mission plan, we might prompt the model first to outline the plan steps briefly (draft), then either let the model expand each step or have the outline approved by human and then expanded. This can be done in one call if the model is instructed to structure its answer (like output a numbered list of phases with one-line each, which itself is succinct). But CoD research suggests maybe doing iterative refinement yields quality with fewer tokens than doing a huge explanation in one go ￼. In real-time 1 Hz operation, iterative drafts per tick is likely unnecessary (since each tick is short anyway). So CoD will mostly apply to larger requests (mission overview, debrief summary, etc.). We will incorporate this by designing those prompts to produce outlines or bullet points first and perhaps allow a follow-up question to detail any bullet if the user requests.
7.	Testing and Tuning: Once these changes are in place, we will rigorously test the agent in various scenarios (the provided ones like baseline crosswind, lesson crosswind, blackout comms, etc.). We’ll check that the context assembly is working (e.g., logs show that when the wind picks up in scenario 2, the lesson from scenario 1 is indeed being pulled in). We’ll check that outputs remain in JSON and are parseable by the UI. We’ll simulate a user query mid-run to see if the agent can handle it without losing context. Also test the CPU mode – perhaps run GPT-OSS-20B quantized or a smaller model to ensure the pipeline still functions. The modular design (OpenAI API compatibility) means we might simply point to a local server or use HuggingFace Transformers pipeline for a quick test on CPU. This ensures our “optional local use” claim is valid – one could run the whole agent on a personal machine (with a smaller model if needed). We highlight this in documentation: e.g., “To run on CPU, set MODEL=ggml-gpt-oss-7b in .env and use aria/agent_local.py instead of Groq service.” This kind of flexibility is a strength.

By following this plan, we integrate cutting-edge context optimization techniques directly into ARIA’s memory fabric, without fundamentally changing the user experience – except that everything becomes faster, more reliable, and more intelligent with each mission. We maintain transparency (the human can always see the reasoning snippet and any retrieved references) and control (human can intervene at every decision), while significantly enhancing the agent’s autonomy and learning capability under the hood.

## Expected Benefits and Innovations

Implementing the above enhancements in ARIA yields numerous benefits, showcasing innovation in agent architecture:
* Extended Context Reasoning: ARIA will handle contexts far exceeding the model’s nominal window by clever memory management. It can remember earlier parts of a long mission or refer to lengthy documents by retrieving relevant pieces on the fly ￼. This means the agent won’t “forget” critical info even in extended operations – a common limitation of vanilla LLM applications.
* Continuous Learning without Retraining: The ability to carry forward lessons ensures that the AI literally gets better with each run. This is a powerful demonstration of learning in context – leveraging the model’s implicit learning ability from examples ￼. Judges can see that the second time a scenario is run, ARIA avoids a mistake or performs more optimally, purely due to the memory mechanism (no weight update). This is cutting-edge, as it hints at a future where fine-tuning might be replaced by sophisticated prompting and memory (as research suggests with concepts like contextual weight updates ￼).
* Efficiency and Speed: Optimizations like Chain-of-Draft and selective reasoning will reduce the token footprint of each model call ￼. Fewer tokens means faster inference (important for a 20B model, even on specialized hardware) and lower computational cost. Adaptive routing ensures we don’t waste a full LLM call on trivial decisions, potentially boosting the system’s responsiveness. The result is an agent that approaches real-time operation even with a large model, which is a tangible win for local AI systems.
* Robustness and Safety: By combining LLM suggestions with rule-based safety checks and by giving the LLM access to authoritative data, ARIA minimizes the chances of harmful or nonsensical outputs. The model’s outputs are always grounded in either the provided context or checked by constraints. In essence, the architecture puts multiple layers of assurance (retrieval, memory of lessons, safety gate) around the LLM. This addresses a key concern in AI autonomy – safety – with a multi-pronged approach (learning from past near-misses, never forgetting rules, and keeping a human in the loop). It shows how an agentic system can be designed responsibly for high-stakes domains.
* Transparency and Auditability: Every decision ARIA makes can be traced: the rationale cites either a past lesson or a doc snippet or refers to a recent event, all of which are stored. This is aligned with the ReAct principle of interleaving reasoning and action ￼ – even if we don’t show the full chain-of-thought to the user, the summarized reasoning and the fact that it’s pulling from explicit sources make the AI’s behavior interpretable. If an issue arises, we can audit the episodic log and see what the AI was “thinking” at each step. This level of traceability is often lacking in end-to-end black-box AI and will impress in a scenario like space missions where accountability is important.
* Flexibility and Portability: Thanks to the model-agnostic design (OpenAI API compatibility), ARIA can be run with different models or on different hardware with minimal changes. It’s future-proof: if tomorrow a better 20B model or a 7B model that runs on a phone comes out, we can integrate it. For the hackathon, the fact that we can showcase it on Groq for speed but also note it can run fully offline on CPU (with smaller model) is a plus for the “local agent” theme. It means the project isn’t just a one-off demo; it could be used by others without requiring cloud or expensive hardware (just with scaled-down performance).
* Human-AI Collaboration: With multi-turn memory and interactive abilities, ARIA truly functions as a collaborative agent rather than a one-shot planner. The human can ask it follow-up questions (“Why did you choose that action?”) or give it new instructions mid-mission (“Avoid area X due to new info”), and ARIA will incorporate that into its memory and subsequent reasoning. The design encourages the AI to seek clarification or help (through ReAct tool use or by deferring to the human if uncertain). This dynamic builds trust – the human is always in control, and the AI acts as a knowledgeable assistant that gets better over time. It’s a compelling demonstration of how AI can augment human decision-makers in complex tasks.
* Excellence in Agentic Architecture: By weaving together ideas from multiple research fronts (RAG, long-term memory, ReAct, CoD, adaptive control) into a cohesive system, ARIA exemplifies an “agentic” architecture that is more than the sum of its parts. Each component (memory, planning, safety, UI) is modular, but the real magic is in how they integrate. This architecture could be generalized to other scenarios (not just space landing). It shows a blueprint of how to build AI agents that are safe, self-improving, and domain-specialized without massive models. In the hackathon context, this hits the marks of innovation, practicality, and impact.

## Conclusion

In conclusion, our plan for ARIA centers on leveraging an advanced memory fabric and context-management strategies to make the most of GPT-OSS-20B’s capabilities. We integrate research-inspired optimizations like MemGPT’s memory hierarchy, ReAct reasoning traces, and Chain-of-Draft efficiency into the existing app structure in a pragmatic way. The result will be an AI agent that can remember, reason, and learn in a manner very similar to a human operator – yet running entirely on local hardware and with a fraction of the resources of giant cloud models. By adapting across missions and improving its suggestions over time, ARIA demonstrates a form of long-term autonomy that is extremely cutting-edge.

In essence, ARIA’s enhanced memory fabric and context optimization not only improve raw performance but also pave the way for a new class of trustworthy, self-improving local AI agents. This is a clear win for both the “Best Local Agent” criteria and the “For Humanity” ethos – as ARIA could one day help save missions (and lives) by providing reliable decision support in critical moments, continuously learning from each success or failure to get better and safer. We believe this architecture showcases innovation that can serve as a foundation for future development in real mission control applications.

Sources: The design and improvements are informed by recent research and frameworks in LLM memory and reasoning, including MemGPT/Letta for virtual context management, the ReAct framework for interleaving reasoning and tool use, the Chain-of-Draft strategy for concise reasoning, and adaptive model routing for efficiency, as well as insights into how in-context learning enables on-the-fly adaptation.